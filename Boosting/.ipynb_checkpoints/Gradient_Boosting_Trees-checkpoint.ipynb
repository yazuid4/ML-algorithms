{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\" font-family=\"Verdana\">\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "Boosting is the process of combining weak learners to generate a __Strong Rule__. A weak learner generate a rule (weak rule) from the training examples, this rule should be better than chance, this means the error should be less than 1/2.\n",
    "\n",
    "* __Formal description of Boosting:__\n",
    "<br><br>\n",
    "   - Given training set  $(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$, each instance  $x \\in X$ we associate the correct label $y_i \\in \\{-1, 1\\}$:\n",
    "<br><br>\n",
    "   - For t = 1,...,T :\n",
    "        - Construct the distribution $D_t$ on $\\{1,...,n\\}$:\n",
    "        - Find weak classifier: $h_t:$ $X \\xrightarrow{} \\{-1, 1\\}$. The error $e_t$ on $D_t$ equal to $Pr_{i \\sim D_t}[h_t(x_i)\\ne y_i]$\n",
    "<br><br>\n",
    "   - Output the final Strong learner $H_\\text{final}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\" font-family=\"Verdana\">\n",
    "    \n",
    "* __Example : AdaBoost__\n",
    "\n",
    "    - The algorithm start by the uniforme distribution: $D_t(i)$ = $\\dfrac{1}{n}$  \n",
    "        - For t = 1,...,T : \n",
    "        - Given $D_t$, $h_t$ (set of weak learners).\n",
    "        <br><br>\n",
    "        - We update the distribution as follows:          \n",
    "$$\n",
    "D_{t+1}(i) = \\dfrac{D_{t}(i)}{Z_t} \\times \\begin{cases} e^{-\\alpha_t} &  y_i = h_t(x_i) \\\\e^{\\alpha_t} &  y_i \\ne h_t(x_i) \\end{cases} = \\dfrac{D_{t}(i)}{Z_t} \\times \\exp(-\\alpha_t y_i h_t(x_i))    \n",
    "$$  \n",
    "          where $Z_t$ normalization factor, $\\alpha_t$ = $\\dfrac{1}{2} \\ln(\\dfrac{1 - e_t}{e_t})$\n",
    "          <br><br>\n",
    "    - The result classifier : $H(x) = sign(\\sum_{t} \\alpha_th_t(x))$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:small;\" font-family=\"Verdana\">\n",
    "    \n",
    "**Notes**:  \n",
    "\n",
    "   1. A week learner can be anything but the popular choice is _Boosting stumps_ ( one-level decision trees ). \n",
    "   <br><br>\n",
    "   2. Adaboost can be viewd as gradient descent over an exponential loss function :  \n",
    "- _Prediction_: $H_t(x) = \\sum_{t} \\alpha_th_t(x)$.  \n",
    "- _Loss_: $L(F_t(x),y)$ Total loss is: $\\sum_{i=1}^{n} L(F_t(x_i),y_i) )$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Application: XGBoost  \n",
    "\n",
    "\n",
    "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Xgboost documentation](https://xgboost.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types whales classification:\n",
    "\n",
    "This example is based on underwater sound data collected in the Gulf of Mexico during and since the time of the Deepwater Horizon oil spill in 2010. The task is to assess what species of whales were present at three recording sites,\n",
    "\n",
    "The classification was performed using the echo-location clicks emitted by beaked whales. These are very short pulses (less than a ms) that whales and dolphins use to sense their surroundings, including the detection of their prey. Different species of beaked whales emit clicks with different waveforms and spectral distributions. The time intervals between clicks also provides information about the species. Examples clicks from Cuvier’s and Gervais’ beaked whales are shown below.\n",
    "\n",
    "\n",
    "<img src=\"./img.jpg\" height=500 width=500>  \n",
    "\n",
    "\n",
    "This notebook don't include data processing  \n",
    "\n",
    "_The goal of the project is to create a classifier that takes as input data a run of clicks and outputs the species._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/X_train.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "with open('Data/X_test.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "\n",
    "with open('Data/y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "with open('Data/y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Parameters for XG Boost  \n",
    "\n",
    "* Maximum Depth of the Tree = 3 _(maximum depth of each decision trees)_\n",
    "* Step size shrinkage used in update to prevents overfitting = 0.3 _(how to weigh trees in subsequent iterations)_\n",
    "* Evaluation Criterion= Maximize Loglikelihood according to the logistic regression _(logitboost)_\n",
    "* Maximum Number of Iterations = 1000 _(total number trees for boosting)_\n",
    "* Early Stop if score on Validation does not improve for 5 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_plst():\n",
    "    param = {}\n",
    "    param['max_depth']= 3   # depth of tree\n",
    "    param['eta'] = 0.3      # shrinkage parameter\n",
    "    param['silent'] = 1     # not silent\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param['nthread'] = 7 # Number of threads used\n",
    "    param['eval_metric'] = 'logloss'\n",
    "    plst = param.items()\n",
    "    return plst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "* The function <font color=\"blue\">calc_stats</font> takes the xgboost margin scores as input and returns two numpy arrays   min_scr, max_scr which are calculated as follows:\n",
    "\n",
    "    1. **min_scr**: mean - (3 $\\times$ std)\n",
    "    2. **max_scr**: mean + (3 $\\times$ std)\n",
    "\n",
    "    Here the input margin scores, represents the processed XGBoost margin scores obtained from the <font color=\"blue\">bootstrap_pred</font> function. Each row represents the various scores for a specific example in an iteration and your <font color=\"blue\">calc_stats</font> function is supposed to compute the **min_scr** and **max_scr** as defined for each example. \n",
    "\n",
    "\n",
    "* The function <font color=\"blue\">predict</font> takes the minimum score array and maximum score array and returns predictions as follows:\n",
    "\n",
    "    1. If respective minimum score and maximum score values are less than 0, predict -1 (**Cuvier's**)\n",
    "    2. If respective minimum score value is less than 0 and maximum score value is greater than 0, predict 0 (**Unsure**)\n",
    "    3. If respective minimum score and maximum score values are greater than 0, predict 1 (**Gervais**)\n",
    "\n",
    "    Based on the ranges for each of the examples, i.e, (min_scr, max_scr), we can predict whether it's a Gervais or a Cuvier. Since we take margin scores from a set of bootstraps for each example, we use the minimum and maximum score arrays to predict to determine the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats(margin_scores):\n",
    "    arr_min_scr = []\n",
    "    arr_max_scr = []\n",
    "    \n",
    "    for arr in margin_scores:\n",
    "        m = arr.mean()\n",
    "        sd = arr.std()\n",
    "        arr_min_scr.append(round(m-3*sd, 2))\n",
    "        arr_max_scr.append(round(m+3*sd, 2))\n",
    "    \n",
    "    return (np.asarray(arr_min_scr), np.asarray(arr_max_scr))\n",
    "\n",
    "\n",
    "def predict(min_scr, max_scr):\n",
    "    \n",
    "    answer = []\n",
    "    \n",
    "    for i in range(len(min_scr)):\n",
    "        if(min_scr[i] < 0 and max_scr[i] < 0):\n",
    "            answer.append(-1)\n",
    "        if(min_scr[i] < 0 and max_scr[i] > 0):\n",
    "            answer.append(0)\n",
    "        if(min_scr[i] > 0 and max_scr[i] > 0):\n",
    "            answer.append(1)\n",
    "            \n",
    "    return np.asarray(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating margins scores\n",
    "\n",
    "#### Main API\n",
    "\n",
    "* `xgboost.train` is the learning API that trains the Gradient Boosting Model,\n",
    "   * The main parameters are:\n",
    "      * **plst** – XGBoost parameter list\n",
    "      * **dtrain** – Data to be trained\n",
    "      * **num_round** – Number of iterations of boosting. (default: 100)\n",
    "      * **evallist** – List of items to be evaluated during training\n",
    "      * **verbose_eval** - This can be used to control how much information the train function prints. You might want to set to **False** to avoid printing logs.\n",
    "* `bst.predict` is the API that makes score predictions\n",
    "   * The main parameters are:\n",
    "      * **dtest** – Test Data\n",
    "      * **dtrain** – Data to be trained\n",
    "      * **ntree_limit** – Limit number of trees in the prediction (Use: ntree_limit=bst.best_ntree_limit)\n",
    "      * **output_margin** - Whether to output the raw untransformed margin value (Use: output_margin=True)\n",
    "\n",
    "\n",
    "#### Procedure\n",
    "\n",
    "Repeat the given procedure for n_bootstrap number of iterations:\n",
    "\n",
    "For **n_bootstrap** iterations:\n",
    "* Sample **boostrap_size** indices from the training set **with replacmennt**\n",
    "* Create train and test data matrices (dtrain, dtest) using xgb.DMatrix(X_sample, label=y_sample)\n",
    "* Initialise the evallist parameter [(dtrain, 'train'), (dtest, 'eval')]\n",
    "* Train the model using the XGBoost train API and make score predictions using bst.predict object returned by XGB train API. Ensure you set **output_margin=True** to get raw untransformed output scores.\n",
    "* Normalize them by dividing them with the normalizing factor as max(scores) - min(scores) and round these values to a precision of two decimal places\n",
    "\n",
    "Then: \n",
    "* For each individual example, remove scores below the minRth percentile and greater than maxRth percentile (sort for each example if necessary)\n",
    "* Call the calc_stats function to compute min_scr and max_scr with the filtered margin scores as parameter\n",
    "* Return the min_scr and max_scr computed by the **calc_stat** function using the margin scores.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_pred(X_train, X_test, y_train, y_test, n_bootstrap, minR,maxR, bootstrap_size, num_round=100, plst=xgboost_plst()):\n",
    "    \n",
    "    answer = []\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        rdc =  np.random.choice(bootstrap_size, bootstrap_size)\n",
    "        xsample = X_train[rdc]\n",
    "        ysample = y_train[rdc]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(xsample, label=ysample)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "        evallist =  [(dtrain, 'train'), (dtest, 'eval')]\n",
    "        \n",
    "        bst = xgb.train(plst, dtrain, num_boost_round=num_round, evals=evallist, early_stopping_rounds=5, verbose_eval=False)\n",
    "        margins_scores = bst.predict(dtest, output_margin=True, ntree_limit=bst.best_ntree_limit, validate_features=True)\n",
    "        \n",
    "        MX = margins_scores.max()\n",
    "        MI = margins_scores.min()\n",
    "        margins_scores = [round(x/(float(MX-MI)),2) for x in margins_scores]\n",
    "        \n",
    "        margins_scores = np.asarray(margins_scores)\n",
    "        sp = margins_scores.shape[0]\n",
    "        margins_scores = margins_scores.reshape(sp,-1)\n",
    "        \n",
    "        if(len(answer) == 0):\n",
    "            answer = np.ones((sp,1)) + maxR\n",
    "        \n",
    "        answer = np.concatenate((answer,margins_scores), axis=1)\n",
    "        \n",
    "        for i in range(len(answer)):\n",
    "            answer[i] = answer[i][(answer[i] < maxR)]\n",
    "        \n",
    "        \n",
    "    return calc_stats(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(X_train, X_test, y_train, y_test, n_bootstrap=100):\n",
    "    min_scr, max_scr = bootstrap_pred(X_train, X_test, y_train, y_test, n_bootstrap=n_bootstrap, \\\n",
    "                                            minR=0.1, maxR=0.9, bootstrap_size=len(X_train))\n",
    "    pred = predict(min_scr, max_scr)\n",
    "    return min_scr, max_scr, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = np.load('Data/vis_indices.npy')\n",
    "X_test_samp = X_test[sample_indices]\n",
    "y_test_samp = np.array(y_test[sample_indices], dtype=int)\n",
    "midpt = np.load('Data/vis_midpt.npy')\n",
    "avg_length = np.load('Data/vis_avg_length.npy')\n",
    "min_scr, max_scr, predictions = process(X_train, X_test_samp, y_train, y_test_samp)\n",
    "length = max_scr - min_scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1, -1, -1,  0,  1,  1, -1,  1,  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24, -0.63, -0.65, -0.63, -0.18,  0.31,  0.24, -0.57,  0.42,\n",
       "       -0.29])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.33, -0.31, -0.12, -0.31,  0.37,  0.61,  0.62, -0.47,  0.54,\n",
       "        0.47])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
