{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means and Intrinsic Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we presente pyspark code to estimate the intrinsic dimension of a dataset using `K-means clustering`.\n",
    "\n",
    "Given a sample set of a synthetically generated dataset, we use the Spark K-means API to estimate the centroids of the data points. After obtaining the centroids of the data, we calculate the mean-squared error of the dataset by calculating the mean-squared distance of each datapoint from it's respective representative centroid. This experiment is to be repeated for different values of K (K being the number of centroids), for the same sample of data. Using the information obtained from these experiments, we can estimate the intrinsic dimension of the sample set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the intrinsic dimension of a data set:\n",
    "\n",
    "Recall that given any $d$ dimensional dataset, we can divide it into $n$ cells of diameter $\\epsilon$ each. The relationship between $n, d, \\epsilon$ is then given by:\n",
    "$$\n",
    "n = \\frac{C}{\\epsilon^d}\n",
    "$$\n",
    "Where $C \\in I\\!R$\n",
    "\n",
    "we may write this as:\n",
    "$$\n",
    "\\log{n} = \\log{C} + d \\times \\log{\\frac{1}{\\epsilon}}\n",
    "$$\n",
    "\n",
    "Given this expression, we can then compute the dimensionality of a dataset using:\n",
    "$$\n",
    "d = \\frac{\\log{n_2} - \\log{n_1}}{\\log{\\epsilon_1} - \\log{\\epsilon_2}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using K-Means to estimate intrinsic dimension:\n",
    "We can use K-Means to approximate the value of intrinsic dimension of a data-set. In this case, the K centers represent the cells, each with a diameter equal to the Mean Squared Distance of the entire dataset to their representative centers. The estimate for intrinsic dimension then becomes:\n",
    "$$\n",
    "d = \\frac{\\log{n_2} - \\log{n_1}}{\\log{\\sqrt{\\epsilon_1}} - \\log{\\sqrt{\\epsilon_2}}} = 2 \\times \\frac{\\log{n_2} - \\log{n_1}}{\\log{\\epsilon_1} - \\log{\\epsilon_2}}\n",
    "$$\n",
    "\n",
    "To learn more about fractional dimensions: [here](https://www.youtube.com/watch?v=gB9n2gHsHN4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from math import log\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "* The function `runKmeans` takes as input the complete dataset rdd, the sample of the rdd on which k-means needs to be run on,    the k value and the count of elements in the complete data-set. It outputs the Mean Squared Distance(MSD) of all the points in the dataset from their closest centers, where the centers are calculated using k-means. To do so we use  **K-Means API**: the initializationMode parameter is set to kmeans++. The computeCost function gives you the sum of squared distances. \n",
    "\n",
    "* The function `omputeIntrinsicDimension` takes as input a pair of values $(n1, e1)$, $(n2, e2)$ and computes the intrinsic dimension as output. $e1, e2$ are the mean squared distances of data-points from their closest center (we use the formula above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runKmeans(data, sample_dataset, k, count):\n",
    "    clusters = KMeans.train(sample_dataset, k, maxIterations=1, initializationMode=\"kmeans++\")\n",
    "    cost = clusters.computeCost(data)\n",
    "    return cost/count\n",
    "\n",
    "def computeIntrinsicDimension(n1, e1, n2, e2):\n",
    "    ans = 2 * (log(n2) - log(n1)) / (log(e1) - log(e2))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run K-Means:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run K-means for various values of k and use these to estimate the intrinsic dimension of the dataset.\n",
    "we run Kmeans only on a subset of 10000 to avoid long time computations. We will be estimating the MSD for K values of 10, 200, 700, 2000. The input dataframe is a parquet file, in order to get the correct approximation of the dimension, we will transform data to array of dense vectors and convert columns to double before running KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "def run(df):\n",
    "    \n",
    "    result = dict()\n",
    "    S = (10000, 20000) #Size of the sample kmeans runs on \n",
    "    list_K = [10, 200, 700, 2000] # k values(number of cells n)\n",
    "    diameters = [] \n",
    "    \n",
    "    rdd1 = df.rdd.map(lambda data: Vectors.dense([float(c) for c in data]))\n",
    "    rdd1_subset = df.limit(S[0]).rdd.map(lambda data: Vectors.dense([float(c) for c in data]))\n",
    "    count = rdd1.count()\n",
    "    \n",
    "    for k in list_K:\n",
    "        d = runKmeans(rdd1, rdd1_subset, k, count)\n",
    "        diameters.append(d)\n",
    "    \n",
    "    # Use the diameters values generated to calculate the intrinsic dimension\n",
    "    # To do so we need just two scales (10-200, 200-700, 700-20000) and their respectives diameters\n",
    "    for i in range(len(list_K)-1):\n",
    "        interdim = computeIntrinsicDimension(list_K[i], diameters[i], list_K[i+1], diameters[i+1])\n",
    "        key = 'ID_' + str(S[0]) + '_' + str(list_K[i]) + '_' + str(list_K[i+1])\n",
    "        result[key] = interdim\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "file_path_small = \"./hw5-small.parquet\"\n",
    "df = spark.read.parquet(file_path_small)\n",
    "ans = run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID_10000_10_200': 1.580327672205145,\n",
       " 'ID_10000_200_700': 1.2730250878501679,\n",
       " 'ID_10000_700_2000': 1.1274508742463538}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
